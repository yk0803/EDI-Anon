{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168be264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from shutil import copyfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "from torchvision.utils import save_image\n",
    "from true_classify import *\n",
    "from Utils import *\n",
    "from anonymization_methods import *\n",
    "from datasets import *\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from collections import Counter\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import xlrd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d283082",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75addd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training and testing data directories\n",
    "source_path = 'F:/Original/All FR/train'\n",
    "\n",
    "class_names = [folder for folder in os.listdir(source_path) if os.path.isdir(os.path.join(source_path, folder))]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "file_list = os.listdir(source_path)\n",
    "model_dir = 'Convnext_pretrained_younas.pt'\n",
    "\n",
    "output_path = 'F:/Anonymized/EDI/Pixel_Masked/Face_Recognition'\n",
    "\n",
    "save_roc_dir = 'F:/Results_Final/ROC/ROC Plots Mouth_Masked/'\n",
    "excel_file_path = 'F:/Results_Final/Excel_Sheets/EDI_All_FR_Results.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a64045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new transform with additional data augmentations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e3817",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.convnext_base(pretrained=True)\n",
    "model.classifier[2]=nn.Linear(1024,num_classes)\n",
    "\n",
    "model.load_state_dict(torch.load('Convnext_pretrained_younas.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d311191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0c83a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will try a fixed q in each iteration\n",
    "q=95\n",
    "blur_kernel_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df8e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, roc_auc, dataset_name, save_dir):\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "\n",
    "    # Plot macro-average ROC curve\n",
    "    plt.plot(fpr['micro'], tpr['micro'], color='deeppink', linestyle=':', lw=lw,\n",
    "             label='Macro-average ROC curve (area = {0:0.2f})'.format(roc_auc['micro']))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC for ' + dataset_name)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.savefig(os.path.join(save_dir, f'ROC_{dataset_name}.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c622f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_points_on_image(image, important_pixels_mask, point_size, point_color):\n",
    "    image = image.to(device)\n",
    "    important_pixels_mask = important_pixels_mask.to(device)\n",
    "\n",
    "    # Apply the mask directly on the GPU tensors\n",
    "    y_indices, x_indices = torch.nonzero(important_pixels_mask, as_tuple=True)\n",
    "    \n",
    "    image_pil = transforms.ToPILImage()(image.squeeze().cpu())\n",
    "    draw = ImageDraw.Draw(image_pil)\n",
    "\n",
    "    for y, x in zip(y_indices, x_indices):\n",
    "        x, y = x.item(), y.item()\n",
    "        x0, y0 = x - point_size, y - point_size\n",
    "        x1, y1 = x + point_size, y + point_size\n",
    "        bbox = [(x0, y0), (x1, y1)]\n",
    "\n",
    "        # Draw a point at the important pixel\n",
    "        draw.rectangle(bbox, fill=point_color)\n",
    "\n",
    "    return transforms.ToTensor()(image_pil).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569d80fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(annonymized_image, i, correct_label, output_path, q):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    ts.save(annonymized_image, os.path.join(output_path, f\"{correct_label.item()}-{i}-q{q}.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489af996",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "new_batch_size = 1\n",
    "iteration_out_path = 'F:/Anonymized/EDI/Pixel_Masked/Face_Recognition'\n",
    "new_test_path = source_path #'F:/Anonymized/EDI/Pixel_Masked/Face_Recognition/Iteration_38'\n",
    "our_test_loader = create_test_loader(new_test_path, new_batch_size)\n",
    "final_acc, correct_examples, labels, logits = test_images_classification(model, device, our_test_loader, excel_file_path, save_roc_dir)\n",
    "prev_acc = final_acc\n",
    "print(prev_acc*100)\n",
    "end_time = time.time()\n",
    "\n",
    "acc_time = end_time - start_time\n",
    "\n",
    "print(f\"Acc time is\", acc_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a428f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for itera in range(3):\n",
    "    # Iterate through all correct examples\n",
    "    for i in range(len(correct_examples)):\n",
    "        x, correct_label, prediction = correct_examples[i], labels[i], logits[i]\n",
    "        y = get_second_largest(logits[i])\n",
    "        y = torch.tensor([y]).to(device)\n",
    "\n",
    "        # Create the output directory for this iteration\n",
    "        iteration_out_path = f\"{output_path}/Iteration_{itera}/\"\n",
    "\n",
    "        class_label = labels[i]\n",
    "        class_subfolder = class_names[class_label]            \n",
    "\n",
    "        class_output_path = os.path.join(iteration_out_path, class_subfolder)\n",
    "        os.makedirs(class_output_path, exist_ok=True)\n",
    "\n",
    "        # Clone the original image and enable gradient computation\n",
    "        annonymized_image = x.clone()\n",
    "        annonymized_image.requires_grad = True\n",
    "\n",
    "        # Calculate the loss based on the model, image, and criterion\n",
    "        output, loss = calculate_loss(model, annonymized_image, y, criterion)\n",
    "\n",
    "\n",
    "        if(output.item() == labels[i].item()):\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            img_grad = annonymized_image.grad.data\n",
    "            \n",
    "            # Optimize the gradients using the quantile value (q)\n",
    "            optimized_gradients = optimize_gradients(img_grad, q)\n",
    "            optimized_gradients = optimized_gradients.to(device)\n",
    "            \n",
    "            # Call the draw_dots_on_image function\n",
    "            annonymized_image = draw_points_on_image(annonymized_image, optimized_gradients[0, 0, :, :] >= 1, point_size=0.05, point_color=(255, 0, 0))\n",
    "\n",
    "            # Save the anonymized image to the output directory\n",
    "            save_image(annonymized_image, i, correct_label, class_output_path, q)  \n",
    "        else:\n",
    "            save_image(annonymized_image, i, correct_label, class_output_path, q)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    our_test_loader = create_test_loader(iteration_out_path, batch_size=1)\n",
    "    accuracy, correct_examples, labels, logits = test_images_classification(model, device, our_test_loader, excel_file_path, save_roc_dir)\n",
    "    print(f\"\\nAccuracy: {accuracy*100} %\")\n",
    "\n",
    "end_time = time.time()\n",
    "Anon_execution_time = end_time - start_time\n",
    "print(f\"\\nAnon Time: {Anon_execution_time} seconds\")\n",
    "print(f\"All images processed for iteration number {itera}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d6231b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bf54d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02769a8f",
   "metadata": {},
   "source": [
    "iteration 5 Accuracy: 37.46586564718733 %\n",
    "iteration 6 Accuracy: 33.28782086291644 %\n",
    "iteration 7 Accuracy: 30.99399235390497 %\n",
    "iteration 8 Accuracy: 28.809393773894048 %\n",
    "iteration 9 Accuracy: 27.2801747678864 %\n",
    "iteration 10 Accuracy: 24.658656471873293 %\n",
    "iteration 11 Accuracy: 23.64827962861824 %\n",
    "iteration 12 Accuracy: 23.64827962861824 %\n",
    "iteration 13 Accuracy: 22.200983069361005 %\n",
    "iteration 14 Accuracy: 18.70562534134353 %\n",
    "iteration 15 Accuracy: 18.37793555434189 %\n",
    "iteration 16 Accuracy: 16.739486619333697 %\n",
    "iteration 17 Accuracy: 15.319497542326596 %\n",
    "iteration 18 Accuracy: 14.964500273074824 %\n",
    "iteration 19 Accuracy: 14.063353358820315 %\n",
    "iteration 20 Accuracy: 12.99836155106499 %\n",
    "iteration 21 Accuracy: 12.261059530311305 %\n",
    "iteration 22 Accuracy: 11.851447296559257 %\n",
    "iteration 23 Accuracy: 11.714909885308575 %\n",
    "iteration 24 Accuracy: 10.677225559803386 %\n",
    "iteration 25 Accuracy: 9.85800109229929 %\n",
    "iteration 26 Accuracy: 9.666848716548335 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f56a207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f757a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc004109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbac14e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de812c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3780889",
   "metadata": {},
   "source": [
    "Accuracy: 53.1403604587657 %\n",
    "\n",
    "Anon Time: 1753.5750188827515 seconds\n",
    "All images processed for iteration number 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde3231b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21c49c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# def resize_and_save(input_path, output_path):\n",
    "#     try:\n",
    "#         # Open the image file\n",
    "#         with Image.open(input_path) as img:\n",
    "#             # Resize the image\n",
    "#             resized_img = img.resize((1024, 1024))\n",
    "\n",
    "#             # Save the resized image\n",
    "#             resized_img.save(output_path)\n",
    "#             print(f\"Image resized and saved to {output_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: {e}\")\n",
    "\n",
    "# # Example usage:\n",
    "# input_image_path = \"D:/Younas_Work/D2_Final/Anonymized/Expite/Masked_New_2/Face_Recognition/Iteration_0/Al Pacino/Original.jpg\"\n",
    "# output_image_path = \"D:/Younas_Work/D2_Final/Anonymized/Expite/Masked_New_2/Face_Recognition/Iteration_0/Al Pacino/Resized.jpg\"\n",
    "\n",
    "# resize_and_save(input_image_path, output_image_path )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
