{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168be264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from shutil import copyfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, f1_score, auc, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from torchvision.utils import save_image\n",
    "from true_classify import *\n",
    "from Utils import *\n",
    "from anonymization_methods import *\n",
    "from datasets import *\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from collections import Counter\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import xlrd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d283082",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75addd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training and testing data directories\n",
    "source_path = 'D:/Younas_Work/D2_Final/Original/All FR/train'\n",
    "\n",
    "class_names = [folder for folder in os.listdir(source_path) if os.path.isdir(os.path.join(source_path, folder))]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "file_list = os.listdir(source_path)\n",
    "model_dir = 'Convnext_pretrained_younas.pt'\n",
    "\n",
    "output_path = 'D:/Younas_Work/D2_Final/Anonymized/Expite/Simple_Pix_Old_15/Face_Recognition'\n",
    "\n",
    "save_roc_dir = 'D:/Younas_Work/D2_Final/Results_Final/ROC/ROC Plots Mouth_Masked/'\n",
    "excel_file_path = 'D:/Younas_Work/D2_Final/Results_Final/Excel_Sheets/Mouth_Masked.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a64045",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a new transform with additional data augmentations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e3817",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.convnext_base(pretrained=True)\n",
    "model.classifier[2]=nn.Linear(1024,num_classes)\n",
    "\n",
    "model.load_state_dict(torch.load('Convnext_pretrained_younas.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d311191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0c83a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will try a fixed q in each iteration\n",
    "q=90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50893a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting Parameter values\n",
    "\n",
    "# we will put all the values to highest possible to minimize the changes.\n",
    "\n",
    "#For Simple Blur\n",
    "blur_kernel_size = 3\n",
    "\n",
    "#For DP Blur\n",
    "#block\n",
    "b = 16\n",
    "#m & eps are for privacy budget. Larger m = more noise\n",
    "m = 2\n",
    "#eps is the privacy parameter in DP, smaller eps means higher noise and more distortion\n",
    "eps = 100\n",
    "#K is kernel size for gaussian blur. Height and width of the Gaussian kernel\n",
    "k = 9\n",
    "#SD of Gaussian blur. Larger sigma means more blurring\n",
    "sigma = 0\n",
    "\n",
    "#For Simple Pixelate\n",
    "pixel_size = 3\n",
    "\n",
    "\n",
    "#For DP Pixelate\n",
    "#b, m and eps are the same for DP pixelate and DP Blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4baab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# new_batch_size = 1\n",
    "# iteration_out_path = 'D:/Younas_Work/SOTA/DP/'\n",
    "# new_test_path = iteration_out_path\n",
    "# our_test_loader = create_test_loader(new_test_path, new_batch_size)\n",
    "# final_acc, correct_examples, labels, logits = test_images_classification(model, device, our_test_loader, excel_file_path, save_roc_dir)\n",
    "# prev_acc = final_acc\n",
    "# print(prev_acc*100)\n",
    "# end_time = time.time()\n",
    "# overall_execution_time = end_time - start_time\n",
    "# print(f\"\\nOverall execution time : {overall_execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489af996",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "new_batch_size = 1\n",
    "iteration_out_path = 'D:/Younas_Work/D2_Final/Anonymized/Expite/Simple_Pix_Old_15/Face_Recognition/Iteration_15'\n",
    "new_test_path = iteration_out_path\n",
    "our_test_loader = create_test_loader(new_test_path, new_batch_size)\n",
    "final_acc, correct_examples, labels, logits = test_images_classification(model, device, our_test_loader, excel_file_path, save_roc_dir)\n",
    "prev_acc = final_acc\n",
    "print(prev_acc*100)\n",
    "\n",
    "end_time = time.time()\n",
    "acc_time = end_time - start_time\n",
    "print(f\"\\nAcc execution time: {acc_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# If the number of iterations crashed your system, try the iterations in batches (batch of 5)\n",
    "for itera in range(20):\n",
    "    # Iterate through all correct examples\n",
    "    for i in range(len(correct_examples)):\n",
    "        x, correct_label, prediction = correct_examples[i], labels[i], logits[i]\n",
    "        y = get_second_largest(logits[i])\n",
    "        y = torch.tensor([y]).to(device)\n",
    "\n",
    "        # Create the output directory for this iteration\n",
    "        iteration_out_path = f\"{output_path}/Iteration_{itera}/\"\n",
    "\n",
    "        class_label = labels[i]\n",
    "        class_subfolder = class_names[class_label]            \n",
    "\n",
    "        # Create subfolder based on the class name\n",
    "    #         class_subfolder = class_names[correct_label.item()]\n",
    "\n",
    "        class_output_path = os.path.join(iteration_out_path, class_subfolder)\n",
    "        os.makedirs(class_output_path, exist_ok=True)\n",
    "\n",
    "        # Clone the original image and enable gradient computation\n",
    "        annonymized_image = x.clone()\n",
    "        annonymized_image.requires_grad = True\n",
    "\n",
    "        # Calculate the loss based on the model, image, and criterion\n",
    "        output, loss = calculate_loss(model, annonymized_image, y, criterion)\n",
    "\n",
    "\n",
    "        if(output.item() == labels[i].item()):\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            img_grad = annonymized_image.grad.data\n",
    "\n",
    "    # Call different perturbation functions here\n",
    "#             perturbed = create_blurred_image(x, blur_kernel_size)\n",
    "            perturbed = create_pixelated_image(x, pixel_size)\n",
    "#             perturbed = create_dp_blurred_image(x, b, m, eps, k, sigma)\n",
    "#            perturbed = create_dp_pixelated_image(x, b, m, eps)\n",
    "\n",
    "\n",
    "            # Calculate noise based on the original and perturbed images\n",
    "            noise = calculate_noise(x, perturbed, device)\n",
    "\n",
    "            # Optimize the gradients using the quantile value (q)\n",
    "            optimized_gradients = optimize_gradients(img_grad, q)\n",
    "            optimized_gradients = optimized_gradients.to(device)\n",
    "\n",
    "            # Update the noise using optimized gradients\n",
    "            updated_noise = noise * optimized_gradients\n",
    "\n",
    "            # Create the anonymized image by subtracting/adding the updated noise from the original image\n",
    "            annonymized_image = annonymized_image - updated_noise\n",
    "\n",
    "            # Save the anonymized image to the output directory\n",
    "            save_image(annonymized_image, i, correct_label, class_output_path, pixel_size, q)  \n",
    "        else:\n",
    "            save_image(annonymized_image, i, correct_label, class_output_path, pixel_size, q)\n",
    "\n",
    "end_time = time.time()\n",
    "Anon_execution_time = end_time - start_time\n",
    "print(f\"\\nAnon Time: {Anon_execution_time} seconds\")\n",
    "print(f\"All images processed for iteration number {itera}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a656a73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
